{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538b0645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# Determine the device to use (GPU if available, otherwise CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ebb81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Detected Intent: add_reminder (Confidence: 0.45)\n",
      "Arabic Detected Intent: add_reminder (Confidence: 0.40)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Load a high-performance multilingual zero-shot classification model with strong Arabic support\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\",  # Top multilingual model for zero-shot tasks with excellent Arabic performance\n",
    "    device=0 if torch.cuda.is_available() else -1,  # Utilize GPU for faster inference\n",
    "    model_kwargs={\"ignore_mismatched_sizes\": True}  # Ignore optional files that may cause 404 errors (fixes the additional_chat_templates issue)\n",
    ")\n",
    "\n",
    "# Define the supported intent categories for the virtual assistant\n",
    "candidate_labels = [\n",
    "    \"greeting\",\n",
    "    \"add_reminder\",\n",
    "    \"view_reminders\",\n",
    "    \"general_question\",\n",
    "    \"delete_reminder\"\n",
    "]\n",
    "\n",
    "# Test with an English input\n",
    "sample_text = \"Remind me to call Ahmed tomorrow at 5 PM\"\n",
    "result = classifier(sample_text, candidate_labels, multi_label=False)\n",
    "print(f\"English Detected Intent: {result['labels'][0]} (Confidence: {result['scores'][0]:.2f})\")\n",
    "\n",
    "# Test with an Arabic input\n",
    "arabic_text = \"ذكرني أتصل بأحمد غدا الساعة 5 مساء\"\n",
    "arabic_result = classifier(arabic_text, candidate_labels, multi_label=False)\n",
    "print(f\"Arabic Detected Intent: {arabic_result['labels'][0]} (Confidence: {arabic_result['scores'][0]:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57e1b9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASSIVE Arabic dataset loaded successfully from local Parquet files.\n",
      "Number of training examples: 11514\n",
      "Available columns: ['id', 'locale', 'partition', 'scenario', 'intent', 'utt', 'annot_utt', 'worker_id', 'slot_method', 'judgments']\n",
      "\n",
      "--- Sample Arabic Example ---\n",
      "Utterance: صحيني تسعة الصباح يوم الجمعة\n",
      "Intent ID: 48\n",
      "Intent Name: alarm_set\n",
      "Annotated Utterance (with slots): صحيني [time : تسعة الصباح] يوم [date : الجمعة]\n",
      "Slots: Not available in this version\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the complete dataset from local Parquet files\n",
    "full_dataset = load_dataset(\"parquet\", data_files=\"massive_data/**/*.parquet\")[\"train\"]\n",
    "\n",
    "# Filter for Arabic examples only (locale = \"ar-SA\")\n",
    "arabic_train = full_dataset.filter(lambda example: example[\"locale\"] == \"ar-SA\")\n",
    "\n",
    "# Create a DatasetDict containing only the Arabic training split\n",
    "# Note: Validation and test splits are not present in the loaded file; training data alone is sufficient for fine-tuning\n",
    "massive_arabic = {\n",
    "    \"train\": arabic_train\n",
    "}\n",
    "\n",
    "# Display loading summary\n",
    "print(\"MASSIVE Arabic dataset loaded successfully from local Parquet files.\")\n",
    "print(f\"Number of training examples: {len(massive_arabic['train'])}\")\n",
    "\n",
    "# Display available columns for verification\n",
    "print(\"Available columns:\", full_dataset.column_names)\n",
    "\n",
    "# Retrieve intent label names for mapping\n",
    "intent_names = full_dataset.features[\"intent\"].names\n",
    "\n",
    "# Display a safe sample from the Arabic training data\n",
    "sample = massive_arabic[\"train\"][0]\n",
    "\n",
    "print(\"\\n--- Sample Arabic Example ---\")\n",
    "print(\"Utterance:\", sample[\"utt\"])\n",
    "print(\"Intent ID:\", sample[\"intent\"])\n",
    "print(\"Intent Name:\", intent_names[sample[\"intent\"]])\n",
    "print(\"Annotated Utterance (with slots):\", sample.get(\"annot_utt\", \"Not available\"))\n",
    "print(\"Slots:\", sample.get(\"slots\", \"Not available in this version\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb6d9f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning on Arabic MASSIVE data (11514 examples)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2880' max='2880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2880/2880 07:53, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.528700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.453400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.367200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.293700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.266800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.311200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.242800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.224700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.239300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.253800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.233000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.221100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.169200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.176600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.136500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.185200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.192100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.153800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.145600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.145100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.086800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.151200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.075400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.109600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.109100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.078300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.078600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.063300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.060300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.045600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.044100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.052700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.066900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.036500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning completed successfully! Model saved in ./arabic_finetuned_model\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Load the full dataset from local Parquet files\n",
    "full_dataset = load_dataset(\"parquet\", data_files=\"massive_data/**/*.parquet\")[\"train\"]\n",
    "\n",
    "# Filter for Arabic examples\n",
    "arabic_dataset = full_dataset.filter(lambda example: example[\"locale\"] == \"ar-SA\")\n",
    "\n",
    "# Map MASSIVE intents to our custom 5 categories\n",
    "intent_mapping = {\n",
    "    \"alarm_set\": \"add_reminder\",\n",
    "    \"alarm_query\": \"view_reminders\",\n",
    "    \"reminder_set\": \"add_reminder\",\n",
    "    \"reminder_query\": \"view_reminders\",\n",
    "    \"calendar_set\": \"add_reminder\",\n",
    "    \"calendar_query\": \"view_reminders\",\n",
    "    \"greeting\": \"greeting\",\n",
    "    \"general_quirky\": \"general_question\",\n",
    "    \"general_question\": \"general_question\",\n",
    "    # Default fallback\n",
    "}\n",
    "\n",
    "def map_intent(example):\n",
    "    original_intent = full_dataset.features[\"intent\"].names[example[\"intent\"]]\n",
    "    example[\"label_str\"] = intent_mapping.get(original_intent, \"general_question\")\n",
    "    return example\n",
    "\n",
    "arabic_dataset = arabic_dataset.map(map_intent)\n",
    "\n",
    "# Create label to ID mapping\n",
    "unique_labels = sorted(set(arabic_dataset[\"label_str\"]))\n",
    "label2id = {label: id for id, label in enumerate(unique_labels)}\n",
    "id2label = {id: label for label, id in label2id.items()}\n",
    "\n",
    "# Add numeric label\n",
    "arabic_dataset = arabic_dataset.map(lambda example: {\"label\": label2id[example[\"label_str\"]]})\n",
    "\n",
    "# Load Arabic tokenizer and model\n",
    "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"utt\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = arabic_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Load model with correct number of labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Training arguments (updated for latest Transformers version)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./arabic_finetuned_model\",\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"no\",  # Fixed: changed from evaluation_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=False,\n",
    "    fp16=True if torch.cuda.is_available() else False  # Mixed precision on GPU\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "print(\"Starting fine-tuning on Arabic MASSIVE data (11514 examples)...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./arabic_finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./arabic_finetuned_model\")\n",
    "\n",
    "print(\"Fine-tuning completed successfully! Model saved in ./arabic_finetuned_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
